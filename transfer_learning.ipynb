{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc077d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import usleep\n",
    "import typing\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f070a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARGS\n",
    "\n",
    "class ARGS(object):\n",
    "    def __init__(self, file, data_per_prediction: int = 128):\n",
    "        self.f = os.path.abspath(file) if not os.path.isabs(file) else file\n",
    "        self.o = self.f.replace(\".edf\",\".npy\")\n",
    "        self.logging_out_path = self.f.replace(\".edf\",\".log\")\n",
    "        \n",
    "        self.auto_channel_grouping =  ['EOG', 'EEG']\n",
    "        self.auto_reference_types  =  None\n",
    "        self.channels              =  ['O1-M2==EEG', 'O2-M1==EEG', 'E1-M1==EOG', 'E2-M1==EOG']\n",
    "        self.data_per_prediction   =  128\n",
    "        self.force_gpus            =  ''\n",
    "        self.header_file_name      =  None\n",
    "        self.model                 =  'u-sleep:1.0'\n",
    "        self.no_argmax             =  True\n",
    "        self.num_gpus              =  0\n",
    "        self.overwrite             =  True\n",
    "        self.project_dir           =  usleep.get_model_path(model_name=self.model.split(\":\")[0], model_version=self.model.split(\":\")[-1])\n",
    "        self.strip_func            =  'trim_psg_trailing'\n",
    "        self.weights_file_name     =  None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95a5a2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utime import Defaults\n",
    "from utime.hyperparameters import YAMLHParams\n",
    "\n",
    "# Load arguments and hyperparamets\n",
    "args = ARGS(file=\"edf_data/9JQY.edf\", data_per_prediction=128)\n",
    "hparams = YAMLHParams(Defaults.get_hparams_path(args.project_dir), no_version_control=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59dd367c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from psg_utils.dataset.sleep_study import SleepStudy\n",
    "from utime.bin.predict_one import get_sleep_study\n",
    "\n",
    "def get_and_load_study(file, args: ARGS, hparams: YAMLHParams) -> SleepStudy:\n",
    "\n",
    "    # Get the sleep study\n",
    "    print(f\"Loading and pre-processing PSG file {file}...\")\n",
    "    hparams['prediction_params']['channels'] = args.channels\n",
    "    hparams['prediction_params']['strip_func']['strip_func_str'] = args.strip_func\n",
    "\n",
    "    study, channel_groups = get_sleep_study(psg_path=file,\n",
    "                                            header_file_name=args.header_file_name,\n",
    "                                            auto_channel_grouping=args.auto_channel_grouping,\n",
    "                                            auto_reference_types=args.auto_reference_types,\n",
    "                                            **hparams['prediction_params'])\n",
    "    \n",
    "    study.channel_groups = channel_groups\n",
    "\n",
    "    return study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54fddeb",
   "metadata": {},
   "source": [
    "## Initialize model to train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cb7d44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (None, 19, 3840, 2)\n",
      "Output shape: (None, 19, 30, 2)\n",
      "Model trainable: True\n",
      "\t dense_classifier_out/kernel:0: ((1, 1, 6, 6))\n",
      "\t dense_classifier_out/bias:0: ((6,))\n",
      "\t sequence_conv_out_1/kernel:0: ((1, 1, 6, 2))\n",
      "\t sequence_conv_out_1/bias:0: ((2,))\n",
      "\t sequence_conv_out_2/kernel:0: ((1, 1, 2, 2))\n",
      "\t sequence_conv_out_2/bias:0: ((2,))\n",
      "Model: \"mU-Sleep\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 19, 3840, 2)]     0         \n",
      "                                                                 \n",
      " uSleep_base (Functional)    (None, 72960, 1, 6)       3119286   \n",
      "                                                                 \n",
      " average_pool (AveragePoolin  (None, 570, 1, 6)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " sequence_conv_out_1 (Conv2D  (None, 570, 1, 2)        14        \n",
      " )                                                               \n",
      "                                                                 \n",
      " sequence_conv_out_2 (Conv2D  (None, 570, 1, 2)        6         \n",
      " )                                                               \n",
      "                                                                 \n",
      " output_reshape (OutputResha  (None, 19, 30, 2)        0         \n",
      " pe)                                                             \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,119,306\n",
      "Trainable params: 62\n",
      "Non-trainable params: 3,119,244\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from utime.bin.evaluate import get_and_load_model, get_and_load_one_shot_model\n",
    "from keras import Model\n",
    "from keras.layers import Input\n",
    "\n",
    "# Initialize model to train\n",
    "hparams = YAMLHParams(Defaults.get_hparams_path(args.project_dir), no_version_control=True)\n",
    "hparams['build']['batch_shape'] = [64, 19, 3840, 2]\n",
    "\n",
    "base = get_and_load_model(\n",
    "            project_dir=args.project_dir,\n",
    "            hparams=hparams,\n",
    "            weights_file_name=hparams.get('weights_file_name')\n",
    "        )\n",
    "clear_output(wait=False)    # Removing glorot intitialization warning...\n",
    "\n",
    "# Freeze base layers\n",
    "base.trainable = False\n",
    "\n",
    "base.layers[-5].trainable = True\n",
    "\n",
    "# Extract base from pre-trained model (remove last )\n",
    "inter_base = Model(inputs=base.input, outputs=base.layers[-5].output, name=\"uSleep_base\")\n",
    "inter_out = inter_base(inputs=base.input, training=False)\n",
    "\n",
    "# Create new head with base model as input with a 2-class problem\n",
    "head=base.create_seq_modeling(in_=inter_out,\n",
    "                            input_dims = base.input_dims,\n",
    "                            data_per_period=args.data_per_prediction,\n",
    "                            n_periods=base.n_periods,\n",
    "                            n_classes=2,\n",
    "                            transition_window=base.transition_window,\n",
    "                            activation=base.activation,\n",
    "                            regularizer=None)\n",
    "\n",
    "\n",
    "model = Model(inputs=base.input, outputs = head, name = \"mU-Sleep\")\n",
    "\n",
    "print(f\"Input shape: {model.input.shape}\")\n",
    "print(f\"Output shape: {model.output.shape}\")\n",
    "print(f\"Model trainable: {model.trainable}\")\n",
    "_=[print(f\"\\t {x.name}: ({x.shape})\") for x in model.trainable_weights]\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2db905f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load some random subjects from train splits\n",
    "with open(\"./splits/skorucack_splits.json\") as f:\n",
    "    splits = json.loads(f.read())\n",
    "\n",
    "\n",
    "dev_studies = [get_and_load_study(f\"edf_data/{x}.edf\", args, hparams) for x in splits['train']]\n",
    "test_studies = [get_and_load_study(f\"edf_data/{x}.edf\", args, hparams) for x in splits['test']]\n",
    "clear_output(wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f313789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 19, 3840, 2)\n",
      "(None, 19, 30, 2)\n",
      "(80, 3840, 4)\n",
      "(80, 30, 2)\n"
     ]
    }
   ],
   "source": [
    "# Let's add some random labels to the studies (for now)\n",
    "from tensorflow import keras\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from scipy.io import loadmat\n",
    "\n",
    "\n",
    "for study in dev_studies:\n",
    "    psg_shape = study.get_all_periods().shape\n",
    "    _y = loadmat(study.psg_file_path.replace(\".edf\",\"_new.mat\"), squeeze_me=True)['x']\n",
    "    _y[np.where(_y != 1)[0]] = 0\n",
    "    _y = to_categorical(_y, num_classes = 2)\n",
    "    study.y = np.reshape(_y, [psg_shape[0], int(psg_shape[1] / study.sample_rate) , 2])\n",
    "    \n",
    "for study in test_studies:\n",
    "    psg_shape = study.get_all_periods().shape\n",
    "    _y = loadmat(study.psg_file_path.replace(\".edf\",\"_new.mat\"), squeeze_me=True)['x']\n",
    "    _y[np.where(_y != 1)[0]] = 0\n",
    "    _y = to_categorical(_y, num_classes = 2)\n",
    "    study.y = np.reshape(_y, [psg_shape[0], int(psg_shape[1] / study.sample_rate) , 2])\n",
    "    \n",
    "\n",
    "print(model.input.shape)\n",
    "print(model.layers[-1].output.shape)\n",
    "print(dev_studies[0].get_all_periods().shape)\n",
    "print(dev_studies[0].y.shape)\n",
    "all_studies = np.concatenate([np.array(dev_studies),np.array(test_studies)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1faaa777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 0.1027 are positive - Naive method baseline is: 0.8973\n"
     ]
    }
   ],
   "source": [
    "dev_pos = sum([np.sum(v.y[...,1]==1) for v in dev_studies])/sum([np.prod(v.y[...,1].shape) for v in dev_studies])\n",
    "dev_neg = 1 - dev_pos\n",
    "\n",
    "print(f\"Training {dev_pos:.4f} are positive - Naive method baseline is: {dev_neg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10f59346",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 6.31 GiB for an array with shape (11600, 19, 3840, 2) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 62\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mon_epoch_end\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m     60\u001b[0m         np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mshuffle(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39midx)\n\u001b[1;32m---> 62\u001b[0m train_data \u001b[39m=\u001b[39m Generator(dev_studies, hparams)  \n",
      "Cell \u001b[1;32mIn[18], line 22\u001b[0m, in \u001b[0;36mGenerator.__init__\u001b[1;34m(self, studies, hparams)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate_indices()\n\u001b[0;32m     21\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39midx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices))\n\u001b[1;32m---> 22\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_x_data()\n\u001b[0;32m     23\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_y_data()\n\u001b[0;32m     25\u001b[0m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mshuffle(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39midx)\n",
      "Cell \u001b[1;32mIn[18], line 50\u001b[0m, in \u001b[0;36mGenerator._get_x_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_psg_by_idx\u001b[39m(s: SleepStudy, period, channel, margin):\n\u001b[0;32m     48\u001b[0m     \u001b[39mreturn\u001b[39;00m s\u001b[39m.\u001b[39mget_all_periods()[period\u001b[39m-\u001b[39mmargin:period\u001b[39m+\u001b[39mmargin\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m,s\u001b[39m.\u001b[39mchannel_groups[channel]\u001b[39m.\u001b[39mchannel_indices]\n\u001b[1;32m---> 50\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49marray([_get_psg_by_idx(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstudies[i[\u001b[39m0\u001b[39;49m]], i[\u001b[39m1\u001b[39;49m], i[\u001b[39m2\u001b[39;49m], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmargin) \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices])\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 6.31 GiB for an array with shape (11600, 19, 3840, 2) and data type float32"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create generator which extracts contigous intervals randomly from a random recording\n",
    "\n",
    "class Generator(tf.keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, studies: typing.List[SleepStudy], hparams: YAMLHParams):\n",
    "        \n",
    "        self.studies = studies\n",
    "        self.params  = hparams\n",
    "        self.batch_shape = hparams['build']['batch_shape']\n",
    "        self.batch_size = self.batch_shape[0]\n",
    "        self.period_size = self.batch_shape[1]\n",
    "        self.n_classes = 2\n",
    "        self.n_channels = 4\n",
    "        self.margin = int(np.floor((self.period_size / 2))) #if self.period_size % 2 else int(self.period_size / 2) - 1\n",
    "        self.num_entries = sum([(x.get_all_periods().shape[0] - self.margin*2) * x.n_channels for x in studies])\n",
    "\n",
    "        # Init x and y\n",
    "        self.indices = self._generate_indices()\n",
    "        self.idx = np.arange(0, len(self.indices))\n",
    "        self.x = self._get_x_data()\n",
    "        self.y = self._get_y_data()\n",
    "\n",
    "        np.random.shuffle(self.idx)\n",
    "   \n",
    "    def __len__(self):\n",
    "        return int(np.floor(self.num_entries / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        inds = self.idx[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        batch_x = self.x[inds]\n",
    "        batch_y = self.y[inds]\n",
    "        return batch_x, batch_y\n",
    "    \n",
    "    def _generate_indices(self):\n",
    "        num_pi=[x.n_periods - self.margin*2 for x in self.studies]\n",
    "        idxs = []\n",
    "        for s in range(len(self.studies)):\n",
    "            for p in range(int(num_pi[s])):\n",
    "                for c in range(self.n_channels):\n",
    "                    idxs.append((s, p+self.margin, c))\n",
    "        return idxs\n",
    "\n",
    "    def _get_x_data(self): \n",
    "        \n",
    "        def _get_psg_by_idx(s: SleepStudy, period, channel, margin):\n",
    "            return s.get_all_periods()[period-margin:period+margin+1,...,s.channel_groups[channel].channel_indices]\n",
    "\n",
    "        return np.array([_get_psg_by_idx(self.studies[i[0]], i[1], i[2], self.margin) for i in self.indices])\n",
    "    \n",
    "    def _get_y_data(self): \n",
    "        \n",
    "        def _get_target_by_idx(s: SleepStudy, period, margin):\n",
    "            return s.y[period-margin:period+margin+1, ...]\n",
    "\n",
    "        return np.array([_get_target_by_idx(self.studies[i[0]], i[1], self.margin) for i in self.indices])\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.idx)\n",
    "\n",
    "# train_data = Generator(dev_studies, hparams)  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8d1d098a",
   "metadata": {},
   "source": [
    "## Train only head (pre fine-tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38caed69",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model = False\n",
    "folder = \"TL_17022023\"\n",
    "\n",
    "if not os.path.exists(folder) and train_model:\n",
    "    os.mkdir(folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79979f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "import pickle\n",
    "\n",
    "epochs_pre = 30\n",
    "#cb = [EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)]\n",
    "\n",
    "\n",
    "if train_model:\n",
    "\n",
    "    model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
    "              loss = tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics = tf.keras.metrics.CategoricalAccuracy())\n",
    "\n",
    "\n",
    "    with tf.device(\"/device:GPU:0\"):\n",
    "        history = model.fit(train_data,\n",
    "                            # validation_data=val_data, \n",
    "                            epochs=epochs_pre,\n",
    "                            # callbacks=cb\n",
    "                            )\n",
    "\n",
    "    # Save training\n",
    "    with open(f\"{folder}\\\\history\", 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)\n",
    "    \n",
    "    model.save_weights(f\"{folder}\\\\weights_new_pre.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df7c10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze all model for fine-tuning and train for another 50 epochs\n",
    "\n",
    "# epochs_post = history.epoch[-1] + 70\n",
    "\n",
    "if train_model:\n",
    "\n",
    "    # Unfreeze for fine-tuning\n",
    "    base.trainable = True\n",
    "\n",
    "    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=1e-7),\n",
    "                loss = tf.keras.losses.CategoricalCrossentropy(),\n",
    "                metrics = tf.keras.metrics.CategoricalAccuracy())\n",
    "\n",
    "    # cb = [EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)]\n",
    "\n",
    "    with tf.device(\"/device:GPU:0\"):\n",
    "        history_fine = model.fit(train_data,\n",
    "                                # validation_data=val_data, \n",
    "                                epochs=epochs_post,\n",
    "                                initial_epoch=history.epoch[-1],\n",
    "                                # callbacks=cb\n",
    "                                )\n",
    "    \n",
    "    # Save fine-tune training\n",
    "    model.save_weights(f\"{folder}\\\\weights_new_post.h5\")\n",
    "\n",
    "    with open(f\"{folder}\\\\history_fine\", 'wb') as file_pi:\n",
    "        pickle.dump(history_fine.history, file_pi)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b73e269e",
   "metadata": {},
   "source": [
    "## Analyze tranfer learning run and make test predictions\n",
    "Files are stored in TL_12192022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3594340",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# folder = \"TL_12192022\"\n",
    "\n",
    "with open(f\"{folder}/history\", \"rb\") as f:\n",
    "    history_pre = pickle.load(f)\n",
    "\n",
    "with open(f\"{folder}/history_fine\", \"rb\") as f:\n",
    "    history_post = pickle.load(f)\n",
    "\n",
    "history = dict.fromkeys(history_post.keys())\n",
    "for (k, pre), (_, post) in zip(history_pre.items(), history_post.items()):\n",
    "    history[k] = [*pre, *post]\n",
    "\n",
    "history['epoch'] = np.arange(0, len(history[k]))\n",
    "history['train_new'] = np.concatenate([np.repeat(\"pre\", len(pre)), np.repeat(\"post\",len(post))])\n",
    "\n",
    "pred_weights = f\"{folder}/weights_new_post.h5\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b386b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot learning curves\n",
    "df = pd.DataFrame(history)\n",
    "change = np.where(df.train_new==\"pre\")[0][-1]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2,1,figsize=(10,6))\n",
    "ax1.plot(df.epoch, df.loss, \"-o\", color=\"tab:blue\", label=\"Train\", markersize=3)\n",
    "ax1.plot(df.epoch, df.val_loss, \"-o\", color=\"tab:orange\", label=\"Validation\", markersize=3)\n",
    "ax1.axvline(x=change, linestyle=\"--\", color=\"tab:green\", label=\"Fine-tuning\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.grid()\n",
    "\n",
    "ax2.plot(df.epoch, df.categorical_accuracy, \"-o\", color=\"tab:blue\", label=\"Training\", markersize=3)\n",
    "ax2.plot(df.epoch, df.val_categorical_accuracy, \"-o\", color=\"tab:orange\", label=\"Validation\", markersize=3)\n",
    "ax2.axvline(x=change, linestyle=\"--\", color=\"tab:green\", label=\"Fine-tuning\")\n",
    "ax2.set_ylabel(\"Accuracy\")\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.legend()\n",
    "ax2.grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69027616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model for prediction\n",
    "\n",
    "def load_prediction_model_for_study(study, weight_file, args, hparams):\n",
    "\n",
    "    base = get_and_load_one_shot_model(\n",
    "                n_periods=study.n_periods,\n",
    "                project_dir=args.project_dir,\n",
    "                hparams=hparams\n",
    "                )\n",
    "    clear_output(wait=False)    # Removing glorot intitialization warning...\n",
    "\n",
    "    # Freeze base layers\n",
    "    base.trainable = False\n",
    "\n",
    "    base.layers[-5].trainable = True\n",
    "\n",
    "    # Extract base from pre-trained model (remove last )\n",
    "    inter_base = Model(inputs=base.input, outputs=base.layers[-5].output, name=\"uSleep_base\")\n",
    "    inter_out = inter_base(inputs=base.input, training=False)\n",
    "\n",
    "    # Create new head with base model as input with a 2-class problem\n",
    "    head=base.create_seq_modeling(in_=inter_out,\n",
    "                                input_dims = base.input_dims,\n",
    "                                data_per_period=args.data_per_prediction,\n",
    "                                n_periods=base.n_periods,\n",
    "                                n_classes=2,\n",
    "                                transition_window=base.transition_window,\n",
    "                                activation=base.activation,\n",
    "                                regularizer=None)\n",
    "\n",
    "\n",
    "    model = Model(inputs=base.input, outputs = head, name = \"mU-Sleep\")\n",
    "\n",
    "    if weight_file is not None:\n",
    "        model.trainable = True  # In order to load weights\n",
    "        model.load_weights(weight_file)\n",
    "        model.trainable = False\n",
    "        \n",
    "\n",
    "    print(f\"Input shape: {model.input.shape}\")\n",
    "    print(f\"Output shape: {model.output.shape}\")\n",
    "    print(f\"Model trainable: {model.trainable}\")\n",
    "    _=[print(f\"\\t {x.name}: ({x.shape})\") for x in model.trainable_weights]\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "def predict_on_study(mdl, study: SleepStudy):\n",
    "\n",
    "    prob = np.empty([len(study.channel_groups), study.n_periods*mdl.output_shape[-2], mdl.output_shape[-1]])\n",
    "    for i, channel_group in enumerate(study.channel_groups):\n",
    "        # Get PSG for particular group\n",
    "        psg = np.expand_dims(study.get_all_periods(),0)\n",
    "        psg_subset = psg[..., tuple(channel_group.channel_indices)]\n",
    "        prob_i = mdl.predict_on_batch(psg_subset)\n",
    "        prob[i,...] = prob_i.reshape(-1, prob.shape[-1])\n",
    "    \n",
    "    return prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab12dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make test predictions\n",
    "\n",
    "test_ids = []\n",
    "test_prob = []\n",
    "test_y = []\n",
    "\n",
    "for s in test_studies:\n",
    "\n",
    "    pred_model = load_prediction_model_for_study(s, weight_file=pred_weights, args=args, hparams=hparams)\n",
    "\n",
    "    prob = predict_on_study(pred_model, s)\n",
    "    \n",
    "    test_ids.append(s.psg_file_path.split(\"\\\\\")[-1].replace(\".edf\",\"\"))\n",
    "    test_prob.append(prob.mean(axis=0))\n",
    "    test_y.append(s.y[...,1].flatten())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b324b40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score, recall_score, precision_score, cohen_kappa_score\n",
    "\n",
    "y_cat_prob = np.concatenate(test_prob)\n",
    "y_cat_true = np.concatenate(test_y)\n",
    "y_pred = y_cat_prob[:,1] >= 0.5\n",
    "\n",
    "print(f\"Confusion matrix:\\n{confusion_matrix(y_cat_true, y_pred)}\")\n",
    "print(f\"F1-Score:\\t{f1_score(y_cat_true, y_pred)}\")\n",
    "print(f\"Precision:\\t{precision_score(y_cat_true, y_pred)}\")\n",
    "print(f\"Recall:\\t\\t{recall_score(y_cat_true, y_pred)}\")\n",
    "# print(f\"Cohen kappa:\\t{cohen_kappa_score(y_cat_true, y_pred):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e4d7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to be used in Matlab\n",
    "\n",
    "from scipy.io import savemat\n",
    "# Write predictions to file\n",
    "test_dict = {\"id\": test_ids,\n",
    "            \"probs\": test_prob,\n",
    "            \"yTrue\": test_y,\n",
    "            \"optThres\": 0.5}\n",
    "\n",
    "savemat(\"Matlab/transfer_learning_new.mat\", test_dict)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77ba22a1",
   "metadata": {},
   "source": [
    "# Debug architecture changes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08c82bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# _base = get_and_load_one_shot_model(\n",
    "#             n_periods=35,\n",
    "#             project_dir=args.project_dir,\n",
    "#             hparams=hparams\n",
    "#             )\n",
    "# clear_output(wait=False)    # Removing glorot intitialization warning...\n",
    "\n",
    "# # Freeze base layers\n",
    "# _base.trainable = False\n",
    "\n",
    "# _base.layers[-5].trainable = True\n",
    "\n",
    "# # Extract base from pre-trained model (remove last )\n",
    "# _inter_base = Model(inputs=_base.input, outputs=_base.layers[-5].output, name=\"uSleep_base\")\n",
    "# _inter_out = _inter_base(inputs=_base.input, training=False)\n",
    "\n",
    "# # Create new head with base model as input with a 2-class problem\n",
    "# head=_base.create_seq_modeling(in_=_inter_out,\n",
    "#                             input_dims = _base.input_dims,\n",
    "#                             data_per_period=args.data_per_prediction,\n",
    "#                             n_periods=_base.n_periods,\n",
    "#                             n_classes=2,\n",
    "#                             transition_window=_base.transition_window,\n",
    "#                             activation=_base.activation,\n",
    "#                             regularizer=None)\n",
    "\n",
    "\n",
    "# _model = Model(inputs=_base.input, outputs = head, name = \"mU-Sleep\")\n",
    "\n",
    "# # _model.trainable = True  # In order to load weights\n",
    "# # _model.load_weights(pred_weights)\n",
    "# # _model.trainable = False\n",
    "    \n",
    "\n",
    "# print(f\"Input shape: {_model.input.shape}\")\n",
    "# print(f\"Output shape: {_model.output.shape}\")\n",
    "# print(f\"Model trainable: {_model.trainable}\")\n",
    "# _=[print(f\"\\t {x.name}: ({x.shape})\") for x in _model.trainable_weights]\n",
    "# _model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0ece51",
   "metadata": {},
   "source": [
    "# Eval training data to check overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc6aed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = []\n",
    "train_prob = []\n",
    "train_y = []\n",
    "\n",
    "for s in dev_studies:\n",
    "\n",
    "    pred_model = load_prediction_model_for_study(s, weight_file=pred_weights, args=args, hparams=hparams)\n",
    "\n",
    "    prob = predict_on_study(pred_model, s)\n",
    "    \n",
    "    train_ids.append(s.psg_file_path.split(\"\\\\\")[-1].replace(\".edf\",\"\"))\n",
    "    train_prob.append(prob.mean(axis=0))\n",
    "    train_y.append(s.y[...,1].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3c1b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write predictions to file\n",
    "train_dict = {\"id\": train_ids,\n",
    "            \"probs\": train_prob,\n",
    "            \"yTrue\": train_y,\n",
    "            \"optThres\": 0.5}\n",
    "\n",
    "savemat(\"Matlab/training_performance_new//transfer_learning_new.mat\", train_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "u-sleep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "c8e95707acf1fc7af41c9420274be1f4b282295d5d7ada269f0cddd16fb8c714"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

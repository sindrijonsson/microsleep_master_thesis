{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2e16f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import typing \n",
    "\n",
    "from IPython.display import clear_output\n",
    "from misc import _get_usleep_token\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Polygon\n",
    "from pprint import pprint\n",
    "from scipy import io, special, stats\n",
    "from sklearn import metrics, model_selection\n",
    "from statistics import mode\n",
    "from tqdm import tqdm\n",
    "from usleep_api import USleepAPI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e093aa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBALS \n",
    "\n",
    "MS_MAPPING = {\"Wake\": 0, \"MS\": 1}\n",
    "AASM_MAPPING = {\"Wake\": 0, \"N1\": 1, \"N2\": 2, \"N3\": 3, \"REM\": 4}\n",
    "\n",
    "float_formatter = \"{:.2f}\".format\n",
    "np.set_printoptions(formatter={'float_kind':float_formatter})\n",
    "\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5750b34b",
   "metadata": {},
   "source": [
    "## Notebook functions\n",
    "1. Helper functions\n",
    "2. Plotting functions\n",
    "3. BernLabels class for handling labelled data from BERN group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0822e9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions \n",
    "\n",
    "def get_probs(file):\n",
    "    out = np.load(file)\n",
    "    probs = special.softmax(out, 1)\n",
    "    return probs\n",
    "\n",
    "def aasm_to_wake_sleep(aasm: np.array):\n",
    "    out = np.zeros(aasm.shape)\n",
    "    out[aasm > 0] = 1\n",
    "    return out\n",
    "\n",
    "\n",
    "def resample_usleep_preds(y_pred: np.array, data_per_pred: int, org_fs: int = 200, usleep_fs: int = 128):\n",
    "    return np.repeat(y_pred, np.floor(data_per_pred * (org_fs / usleep_fs)),0) \n",
    "\n",
    "\n",
    "def load_pickle_from_file(file):\n",
    "    with open(file, 'rb') as handle:\n",
    "        return pickle.load(handle)\n",
    "    \n",
    "def write_to_pickle_file(obj, file):\n",
    "    with open(file, 'wb') as handle:\n",
    "        pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def _find_singles(y,idx):\n",
    "    \n",
    "    pidx = idx-1\n",
    "    if pidx[0] < 0:\n",
    "        prev = y[pidx[1:]]\n",
    "        prev = np.append(0,prev)\n",
    "    else:\n",
    "        prev = y[pidx]\n",
    "\n",
    "    nidx = idx+1\n",
    "    if nidx[-1] >= len(y):\n",
    "        nxt = y[nidx[:-1]]\n",
    "        nxt = np.append(nxt, 0)\n",
    "    else:\n",
    "        nxt = y[nidx]\n",
    "\n",
    "    single_idx = np.logical_not(prev) * np.logical_not(nxt)\n",
    "    singles = idx[single_idx]\n",
    "    return singles\n",
    "\n",
    "\n",
    "def get_target_label_start_and_stop_indices(labels, target):\n",
    "\n",
    "    # Find indices of the target in labels\n",
    "    idx = np.where(labels==target)[0]\n",
    "\n",
    "    # Calculate where there are islands of target in labels\n",
    "    islands = np.diff(idx)==1\n",
    "    ffy = np.pad(islands, pad_width=(1, 1), mode=\"constant\", constant_values=(0,0)).astype(int)\n",
    "\n",
    "    # Calculate where the islands start (>0) and end (<0) by calculating the differnece of the island indices\n",
    "    diff_idx = np.diff(ffy)\n",
    "    \n",
    "    # Find where they start and end\n",
    "    start_idx = np.where(diff_idx > 0)[0]\n",
    "    stop_idx = np.where(diff_idx < 0)[0]\n",
    "\n",
    "    start = idx[start_idx]\n",
    "    stop = idx[stop_idx] \n",
    "    \n",
    "    # Also find single targets\n",
    "    if np.any(idx):\n",
    "        singles = _find_singles(labels, idx)\n",
    "    else:\n",
    "        singles = np.empty([0])\n",
    "        \n",
    "    return start, stop, singles\n",
    "\n",
    "def remove_invalid_labels(labels, target = 1, min_duration = 3, max_duration = 15, fs = 1, verbose = True):\n",
    "    \n",
    "    start, stop, singles = get_target_label_start_and_stop_indices(labels, target)\n",
    "    target_time = (stop - start) / fs\n",
    "    \n",
    "    too_short = np.where(target_time < min_duration)[0]\n",
    "    too_long = np.where(target_time > max_duration)[0]\n",
    "    \n",
    "    unit = \"samples\" if fs == 1 else \"seconds\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"{len(too_short)} labels are shorter than {min_duration} {unit}\\n\"\n",
    "              f\"{len(too_long)} labels are longer than {max_duration} {unit}\\n\")\n",
    "\n",
    "    invalid_idx = np.hstack([too_short, too_long])\n",
    "    \n",
    "    fixed_labels = labels\n",
    "    for i in invalid_idx:\n",
    "        fixed_labels[start[i]:stop[i]+1] = 0 \n",
    "    \n",
    "    if np.any(singles):\n",
    "        if verbose:\n",
    "            print(f\"{len(singles)} singletons were found and removed\") \n",
    "        fixed_labels[singles] = 0\n",
    "        \n",
    "    \n",
    "    return fixed_labels\n",
    "\n",
    "#def _fill_label_gaps(labels, target, limit, fs = 1):\n",
    "#    \n",
    "#    start, stop = get_target_label_start_and_stop_indices(labels, target)\n",
    "#    label_gap = (start[1:] - stop[0:-1]) / fs\n",
    "#    idx = np.where(ms_gap <= limit)[0]\n",
    "\n",
    "#    filled_gaps = np.copy(labels)\n",
    "#    for i in idx:\n",
    "#        filled_gaps[stop[i]+1:start[i+1]] = 1\n",
    "#    \n",
    "#    return filled_gaps\n",
    "\n",
    "def rolling_window(array, window_size,freq):\n",
    "    shape = (array.shape[0] - window_size + 1, window_size)\n",
    "    strides = (array.strides[0],) + array.strides\n",
    "    rolled = np.lib.stride_tricks.as_strided(array, shape=shape, strides=strides)\n",
    "    return rolled[np.arange(0,shape[0],freq)]\n",
    "\n",
    "def plot_roc_curve(y_true, y_probs, pos_label=1, ax = None):\n",
    "    \n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "    \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_true, y_probs[:,pos_label], pos_label = pos_label)\n",
    "    \n",
    "    ax.plot([0,1],[0,1])\n",
    "    ax.plot(fpr, tpr)\n",
    "    ax.set_xlabel(\"False Positive Rate (FPR)\")\n",
    "    ax.set_ylabel(\"True Positive Rate (TPR)\")\n",
    " \n",
    "    return fpr, tpr, ax\n",
    "\n",
    "\n",
    "def plot_precision_recall_curve(y_true, y_probs, pos_label=1, ax = None):\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "    \n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y_true, y_probs[:,pos_label], pos_label = pos_label)\n",
    "    \n",
    "    if ax is not None:\n",
    "\n",
    "        baseline = np.sum(y_true == pos_label) / len(y_true) \n",
    "        ax.plot([0,1],[baseline, baseline], linestyle='--')\n",
    "        ax.plot(recall, precision)\n",
    "        ax.set_xlabel(\"Recall\")\n",
    "        ax.set_ylabel(\"Precision\")\n",
    "\n",
    "    return precision, recall, ax\n",
    "\n",
    "\n",
    "\n",
    "def compute_performance_metrics(y_true, y_pred, y_probs,\n",
    "                                labels = [0,1], classes = [\"Wake\",\"MS\"],\n",
    "                                minority_label = 1, pos_label = 1, plot_on = True):\n",
    "    \n",
    "    if plot_on:\n",
    "        fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=[9,4])\n",
    "    else:\n",
    "        ax1 = None\n",
    "        ax2 = None\n",
    "    \n",
    "    # Get general classification report\n",
    "    report = metrics.classification_report(y_true, y_pred, labels=labels, target_names=classes,\n",
    "                                           output_dict=True)\n",
    "     \n",
    "    # Calculate ROC metics    \n",
    "    roc_fpr, roc_tpr, roc_ax = plot_roc_curve(y_true, y_probs, pos_label, ax1)\n",
    "    roc_auc = metrics.roc_auc_score(y_true, y_probs[:,pos_label])\n",
    "    \n",
    "    # Calculate PR curve metrics\n",
    "    _precision, _recall, pr_ax = plot_precision_recall_curve(y_true, y_probs, pos_label, ax2)\n",
    "    pr_auc = metrics.auc(_recall, _precision)\n",
    "    \n",
    "    \n",
    "    # Compute Matthews correlation coefficent\n",
    "    mcc = metrics.matthews_corrcoef(y_true, y_pred)\n",
    "    \n",
    "    # Compute Cohen's Kappa\n",
    "    cohen_kappa = metrics.cohen_kappa_score(y_true, y_pred)\n",
    "    \n",
    "    # Store results in report\n",
    "    report[\"roc_auc\"]     = roc_auc\n",
    "    report[\"pr_auc\"]      = pr_auc\n",
    "    report[\"mcc\"]         = mcc\n",
    "    report[\"cohen_kappa\"] = cohen_kappa\n",
    "    \n",
    "    # A bit extra spicy\n",
    "    if plot_on:\n",
    "        ax1.annotate(f\"ROC AUC = {roc_auc:.2f}\", (0.7,0.2), fontsize=8)\n",
    "        ax2.annotate(f\"PR AUC = {pr_auc:.2f}\", (0.7,0.7), fontsize=8)\n",
    "    \n",
    "    return report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "244158eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting functions\n",
    "\n",
    "def format_ax(ax, labs):\n",
    "    ax.set_xlabel(\"Period number\")\n",
    "    ax.set_ylabel(\"Sleep stage\")\n",
    "    ax.set_yticks(range(len(labs)))\n",
    "    ax.set_yticklabels(labs)\n",
    "    ax.invert_yaxis()\n",
    "    line = ax.lines[0]\n",
    "    ids = line.get_xdata()\n",
    "    ax.set_xlim(1, ids[-1]+1)\n",
    "    l = ax.legend(loc=3)\n",
    "    l.get_frame().set_linewidth(0)\n",
    "\n",
    "def ghost_poly(face_color=[1,1,1], edge_color=[0,0,0]):\n",
    "    _xy = np.empty([4,2])\n",
    "    _xy[:] = np.nan\n",
    "    _poly = Polygon(_xy, facecolor=face_color, edgecolor=edge_color)\n",
    "    return _poly\n",
    "\n",
    "def plot_probs(ax, probs: np.array, labs, fs = 1):\n",
    "\n",
    "    av = np.cumsum(probs, axis=1)\n",
    "    c = sns.color_palette(\"tab10\", len(labs)-1)\n",
    "\n",
    "    # Create 'ghost' patch for 'Wake'\n",
    "    _poly = ghost_poly()\n",
    "    ax.add_patch(_poly)\n",
    "\n",
    "    for i in range(probs.shape[1]-1):\n",
    "        xy = np.zeros([av.shape[0] * 2, 2]) \n",
    "        xy[:av.shape[0], 0] = np.arange(av.shape[0]) / fs\n",
    "        xy[av.shape[0]:, 0] = np.flip(np.arange(av.shape[0]), axis=0) / fs\n",
    "        xy[:av.shape[0], 1] = av[:, i]\n",
    "        xy[av.shape[0]:, 1] = np.flip(av[:, i+1], axis=0)\n",
    "\n",
    "        poly = Polygon(xy, facecolor=c[i], edgecolor=None)\n",
    "        ax.add_patch(poly)\n",
    "\n",
    "    ax.set_ylabel(\"Probability\")\n",
    "    ax.legend(labs, loc='lower left')\n",
    "    ax.set_xlim([0, av.shape[0]])\n",
    "\n",
    "def plot_label_patches(labels, mapping, y_min=0, y_max=1, ax = None, fs = 1):\n",
    "    \n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "    \n",
    "    def create_patches(ax, start, stop, cc, y_min=0, y_max=1, fs = 1):\n",
    "        for start_idx, stop_idx in zip(start, stop):\n",
    "\n",
    "            x = np.arange(start_idx, stop_idx+1) / fs\n",
    "            xn = len(x)\n",
    "            xy = np.zeros([xn * 2, 2])\n",
    "            xy[:xn, 0] = x\n",
    "            xy[xn:, 0] = np.flip(x, axis=0)\n",
    "\n",
    "            xy[:xn, 1] = np.ones([xn]) * y_min\n",
    "            xy[xn:, 1] = np.ones([xn]) * y_max\n",
    "            poly = Polygon(xy, facecolor=cc, edgecolor=None, alpha=1)\n",
    "            patch = ax.add_patch(poly)\n",
    "\n",
    "        return patch\n",
    "    \n",
    "    handles = []\n",
    "    handle_names = []\n",
    "    colors = sns.color_palette(\"tab10\", len(mapping.keys())-1)\n",
    "    for label_name, label in mapping.items():\n",
    "        \n",
    "        if label == 0 or label_name == \"Wake\":\n",
    "            _poly = ghost_poly()\n",
    "            handle = ax.add_patch(_poly)\n",
    "            handles.append(handle)\n",
    "            handle_names.append(label_name)\n",
    "            continue\n",
    "        \n",
    "        start, stop, singles = get_target_label_start_and_stop_indices(labels, label)\n",
    "        # Convert to time axis with fs\n",
    "        start = start \n",
    "        stop = stop \n",
    "        if not np.any(start):\n",
    "            print(\"No start\")\n",
    "            continue\n",
    "        else:\n",
    "            handle = create_patches(ax, start, stop, colors[label-1], y_min, y_max, fs = fs)\n",
    "            handles.append(handle)\n",
    "            handle_names.append(label_name)\n",
    "            \n",
    "        if np.any(singles):\n",
    "            for x in singles:\n",
    "                _ = ax.axvline(x=x/fs, ymin=y_min, ymax=y_max, color=colors[label-1])\n",
    "        \n",
    "    return handles, handle_names\n",
    "\n",
    "\n",
    "def MatplotlibClearMemory():\n",
    "    allfignums = plt.get_fignums()\n",
    "    for i in allfignums:\n",
    "        fig = plt.figure(i)\n",
    "        fig.clear()\n",
    "        plt.close( fig )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ba6b908e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernLabels(object):\n",
    "    \n",
    "    fs            = 200\n",
    "    folder        = \"labels\"\n",
    "    raw_mapping   =  {\"Wake\": 0,\n",
    "                      \"MSE\": 1,\n",
    "                      \"MSEc\": 2,\n",
    "                      \"ED\": 3}\n",
    "    \n",
    "    min_duration = 3\n",
    "    max_duration = 15\n",
    "    \n",
    "    \n",
    "    def __init__(self, file: str, mapping: dict, include_unilateral = False):\n",
    "        \n",
    "        # File settings and read\n",
    "        self.file = file\n",
    "        self.path = os.path.join(self.folder, file) if not os.path.exists(self.file) else file\n",
    "        self.__raw = io.loadmat(self.path)\n",
    "        \n",
    "        # Disseminate raw data\n",
    "        self.raw_O1 = np.squeeze(self.__raw['labels']['O1'][0][0])\n",
    "        self.raw_O2 = np.squeeze(self.__raw['labels']['O2'][0][0])\n",
    "        self.raw_labels = np.vstack([self.raw_O1, self.raw_O2])\n",
    "        self.num_labels = self.raw_labels.shape[1]\n",
    "        self.time = np.arange(0, (self.num_labels)/self.fs, 1/self.fs)\n",
    "        \n",
    "        # Apply mapping (i.e. convert Bern to Wake vs Sleep)\n",
    "        self.mapping = mapping\n",
    "        self.include_unilateral = include_unilateral\n",
    "        self.label_mapping = {k: i for i, k in enumerate(self.mapping.keys())}\n",
    "        self.convert_labels(mapping, include_unilateral)\n",
    "        \n",
    "        \n",
    "    def __repr__(self): \n",
    "        cls = self.__class__.__name__\n",
    "        return f\"{cls}(file='{self.file}', mapping={self.mapping}, include_bilateral={self.include_unilateral})\"\n",
    "    \n",
    "    def append(self, other):\n",
    "        \n",
    "        assert self.mapping == other.mapping\n",
    "        assert self.include_unilateral == other.include_unilateral\n",
    "        \n",
    "        self.file = [self.file, other.file]\n",
    "        self.__raw = [self.__raw, other.__raw]\n",
    "        func = lambda x, y: np.hstack([x,y])\n",
    "        self.raw_O1 = func(self.raw_O1, other.raw_O2)\n",
    "        self.raw_O2 = func(self.raw_O2, other.raw_O2)\n",
    "        self.raw_labels = func(self.raw_labels, other.raw_labels)\n",
    "        self.num_labels = self.num_labels + other.num_labels\n",
    "        self.time = np.arange(0, (self.num_labels)/self.fs, 1/self.fs) #Incorrect but needed for plotting hypnograms\n",
    "        \n",
    "        self.labels = func(self.labels, other.labels)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def convert_labels(self, mapping = dict, include_unilateral = False):\n",
    "        \n",
    "        print_on = False\n",
    "        \n",
    "        self.labels = np.empty(self.raw_O1.shape)\n",
    "        \n",
    "        if include_unilateral: \n",
    "            func = lambda x: np.any(x, 0)\n",
    "        else:\n",
    "            func = lambda x: np.all(x, 0)\n",
    "            \n",
    "        wake_idx = func(np.isin(self.raw_labels, mapping[\"Wake\"])) \n",
    "        sleep_idx = func(np.isin(self.raw_labels, mapping[\"MS\"]))\n",
    "        \n",
    "        total_idx = (np.sum(wake_idx)+np.sum(sleep_idx))\n",
    "        \n",
    "        if (((total_idx) != self.num_labels) and print_on):\n",
    "            print(f\"Warning! {self.num_labels - total_idx} labels disagree... \\nWill be overwritten as Wake\")\n",
    "            print(self.raw_labels[:,self.raw_O1 != self.raw_O2].T)\n",
    "        \n",
    "        self.labels[wake_idx] = self.label_mapping[\"Wake\"]\n",
    "        self.labels[sleep_idx] = self.label_mapping[\"MS\"]\n",
    "        \n",
    "        self.mapping = mapping\n",
    "        self.include_unilateral = include_unilateral\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def apply_time_critera(self, min_duration = 3, max_duration = 15, replace = True):\n",
    "        \n",
    "        ms_index = self.label_mapping[\"MS\"]\n",
    "        copy_labels = np.copy(self.labels)\n",
    "        fixed_labels = remove_invalid_labels(copy_labels, target=ms_index,\n",
    "                                             min_duration = min_duration, max_duration = max_duration,\n",
    "                                             fs = self.fs)\n",
    "        \n",
    "        if replace:\n",
    "            print(\"Overwriting labels!\")\n",
    "            self.labels = fixed_labels\n",
    "            return\n",
    "        else:\n",
    "            return fixed_labels\n",
    "        \n",
    "    def apply_rolling_func(self, win = 0.2, step = 0.2, func=None, replace = False):\n",
    "        y = np.copy(self.labels)\n",
    "        win_samples = int(win * self.fs)\n",
    "        step_samples = int(step * self.fs)\n",
    "        arr = np.array(rolling_window(y, win_samples, step_samples))\n",
    "        \n",
    "        if func is None:\n",
    "            y_func = np.median(arr,1)\n",
    "        else:\n",
    "            y_func = func(arr)\n",
    "            \n",
    "        if replace:\n",
    "            self.labels = y_func\n",
    "            self.prev_fs = self.fs\n",
    "            self.fs = 1/step\n",
    "        else:\n",
    "            return (y_func, rolling_window(self.time, win_samples, step_samples))\n",
    "    \n",
    "    def get_labels_per_num_seconds(self, num, func = lambda x: mode(x)):\n",
    "    \n",
    "        samps_per_label = self.fs * num\n",
    "        labels_reshaped = np.reshape(self.labels, [int(len(self.labels)/samps_per_label),samps_per_label])\n",
    "        tmp = [func(x) for x in b]\n",
    "        tmp = np.array(tmp)\n",
    "        return np.repeat(c,self.fs*num)\n",
    "    \n",
    "    def plot_raw_labels(self, ax = None, as_hypnogram = False, fs=200):\n",
    "        \n",
    "        \n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots()\n",
    "\n",
    "        if as_hypnogram:\n",
    "            ids = np.arange(self.num_labels)\n",
    "            ax.step(self.time, self.raw_O1, \"-\", linewidth=1, color=\"darkred\", label=\"Raw [O1]\")\n",
    "            ax.step(self.time, self.raw_O2, \"--\", linewidth=1, color=\"darkgray\", label=\"Raw [O2]\")\n",
    "            ax.legend()\n",
    "            format_ax(ax, self.raw_mapping.keys())\n",
    "        \n",
    "        else:\n",
    "            _, _ = plot_label_patches(labels=self.raw_O1, mapping=self.raw_mapping, fs=fs,\n",
    "                                                         y_min=0, y_max=0.5, ax=ax)\n",
    "            _, _ = plot_label_patches(labels=self.raw_O2, mapping=self.raw_mapping, fs=fs,\n",
    "                                                         y_min=0.5, y_max=1, ax=ax,)\n",
    "            ax.autoscale(enable=True, axis = \"both\", tight = True)\n",
    "            ax.plot([*ax.get_xlim()],[0.5, 0.5],'k-',linewidth=0.5)\n",
    "            handles = self.__proxy_legend(ax)\n",
    "            ax.legend(handles, self.raw_mapping.keys())\n",
    "            ax.set_yticks([0.25, 0.75])            \n",
    "            ax.set_yticklabels([\"Raw O1\",\"Raw O2\"])\n",
    "            ax.set_ylim([0, 1])\n",
    "        \n",
    "        xlab = \"Time [s]\" if fs == self.fs else \"Sample #\"\n",
    "        ax.set_xlabel(xlab)\n",
    "        ax.autoscale(enable=True, axis = \"both\", tight = True)\n",
    "        return ax\n",
    "            \n",
    "        \n",
    "    def plot_labels(self, ax = None, as_hypnogram = False, fs = 200):\n",
    "        \n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots()\n",
    "        \n",
    "        if as_hypnogram:\n",
    "            ids = np.arange(self.num_labels)\n",
    "            ax.step(self.time, self.labels, \"-\", linewidth = 1, color = \"darkblue\", label=\"New Labels\")\n",
    "            ax.legend()\n",
    "            format_ax(ax, self.mapping.keys())\n",
    "            ax.set_xlabel(\"Time [s]\")\n",
    "        \n",
    "        else:\n",
    "            hdl, names = plot_label_patches(labels=self.labels, mapping=self.label_mapping, ax=ax, fs = fs)\n",
    "            ax.legend(hdl, names, loc=\"upper left\")\n",
    "            ax.set_yticks([0.5])\n",
    "            ax.set_yticklabels([\"New labels\"])\n",
    "        \n",
    "        xlab = \"Time [s]\" if fs == self.fs else \"Sample #\"\n",
    "        ax.set_xlabel(xlab)\n",
    "        ax.autoscale(enable=True, axis = \"both\", tight = True)\n",
    "        return ax\n",
    "        \n",
    "        \n",
    "    def __proxy_legend(self, ax):\n",
    "        _colors = sns.color_palette(\"tab10\", len(self.raw_mapping.keys())-1)\n",
    "        _handles = [ax.add_patch(ghost_poly())]\n",
    "        for i in range(len(self.raw_mapping.keys())-1):\n",
    "            _poly = ghost_poly(face_color=_colors[i], edge_color=[1,1,1])\n",
    "            _patch = ax.add_patch(_poly)\n",
    "            _handles.append(_patch)\n",
    "        return _handles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06762e6",
   "metadata": {},
   "source": [
    "## Subset data\n",
    "Select a subset (n = 5) of data to use for pre-liminary analysis of microsleep predictions using a fully trained U-Sleep model.\n",
    "Set the seed to 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "4a822f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_edf_files = np.array(sorted(os.listdir(\"edf_data/\")))\n",
    "all_labels_files = np.array(sorted(os.listdir(\"labels/\")))\n",
    "all_names = [x.replace(\".edf\",\"\") for x in all_edf_files]\n",
    "assert [x.replace(\".edf\", \"\") for x in all_edf_files] == [x.replace(\".mat\",\"\") for x in all_labels_files]\n",
    "\n",
    "use_bern_splits = True\n",
    "\n",
    "if use_bern_splits:\n",
    "    with open(\"skorucack_splits.json\",\"r\") as f:\n",
    "        splits = json.loads(f.read())\n",
    "\n",
    "    train_names = np.array(splits['train'])\n",
    "    train_idx = np.isin(all_names, train_names)\n",
    "    test_names = np.array(splits['test'])\n",
    "    test_idx = np.isin(all_names, val_names)\n",
    "    test_edf_files = all_edf_files[test_idx]\n",
    "    test_label_files = all_labels_files[test_idx]\n",
    "else:    \n",
    "    random.seed(42)\n",
    "    n_files = 5\n",
    "    train_idx = random.sample(range(0, len(all_edf_files)), n_files)\n",
    "\n",
    "\n",
    "edf_files = all_edf_files[train_idx]\n",
    "label_files = all_labels_files[train_idx]\n",
    "names = [x.replace(\".edf\", \"\") for x in edf_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e827d93c",
   "metadata": {},
   "source": [
    "## Preliminary analysis\n",
    "* Run all pre-liminary recordings through U-Sleep API\n",
    "* Read and concatinate output files\n",
    "* Compute statistics\n",
    "    * ROC Curves and ROC AUC \n",
    "    * PR Curves and PR AUC\n",
    "    * Cohen's kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ccb642",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ut_commands import predict_one\n",
    "input_dir              = \"edf_data\"\n",
    "output_dir             = \"predictions\"\n",
    "\n",
    "out_files = []\n",
    "samples_per = [1,2,4,8,16]\n",
    "for s in samples_per:\n",
    "    hz = int(128/s)\n",
    "    for edf in all_edf_files:\n",
    "        name = edf.replace(\".edf\",\"\")\n",
    "        inp_file = os.path.join(input_dir, edf)\n",
    "        out_file = os.path.join(output_dir, f\"{hz}_hz\", f\"{name}.npy\")\n",
    "        out_files.append(out_file)\n",
    "\n",
    "        #predict_one(input_file = inp_file, output_file=out_file, data_per_prediction=s)\n",
    "        clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ed588c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set some parameters\n",
    "model                  = \"U-Sleep v1.0\"\n",
    "input_dir              = \"edf_data\"\n",
    "output_dir             = \"predictions\"\n",
    "predictions_per_second = 1\n",
    "channel_groups         = [['EEG O1-M2', 'EOG LOC-M1'],\n",
    "                          ['EEG O2-M1', 'EOG LOC-M1'],\n",
    "                          ['EEG O1-M2', 'EOG ROC-M1'],\n",
    "                          ['EEG O2-M1', 'EOG ROC-M1']]\n",
    "#try:\n",
    "#    api = USleepAPI(api_token=os.environ[\"USLEEP_API_TOKEN\"])\n",
    "#except:\n",
    "#    api = USleepAPI(api_token=_get_usleep_token())\n",
    "\n",
    "out_files = []\n",
    "pred_window = str(int(predictions_per_second**-1)) + \"sec\"\n",
    "for edf in edf_files:\n",
    "    \n",
    "    name = edf.replace(\".edf\",\"\")\n",
    "    \n",
    "    inp_file = os.path.join(input_dir, edf)\n",
    "    out_file = os.path.join(output_dir, f\"{predictions_per_second}_hz\",f\"{name}.npy\")\n",
    "    print(f\"Running: {out_file}\")\n",
    "    \n",
    "    #api.quick_predict(\n",
    "    #     input_file_path=inp_file,\n",
    "    #     model = model,\n",
    "    #     output_file_path=out_file,\n",
    "    #     data_per_prediction=128 * (1/predictions_per_second),\n",
    "    #     channel_groups=channel_groups,\n",
    "    #     with_confidence_scores=True\n",
    "    # )\n",
    "\n",
    "    out_files.append(out_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca5a783",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "y_true = np.empty(shape=[0])\n",
    "y_probs = np.empty(shape=[0,5])\n",
    "\n",
    "mapping = {\"Wake\": [0, 2, 3], \"MS\": [1]}\n",
    "uni_on = True\n",
    "replace = False\n",
    "pred_freq = 1\n",
    "\n",
    "for idx, rec in enumerate(out_files):\n",
    "    \n",
    "    probs = get_probs(rec)\n",
    "    probs_resampled = resample_usleep_preds(probs, data_per_pred = 1)#128 * (1/predictions_per_second))\n",
    "    y_probs = np.concatenate([y_probs, probs_resampled])\n",
    "    #y_probs = np.concatenate([y_probs, probs])\n",
    "    \n",
    "    if idx == 0:\n",
    "        BERN = BernLabels(label_files[idx], mapping, uni_on)\n",
    "        BERN.apply_time_critera(replace=replace)\n",
    "    else:\n",
    "        tmp = BernLabels(label_files[idx], mapping, uni_on)\n",
    "        tmp.apply_time_critera(replace=replace)\n",
    "        BERN.append(tmp)\n",
    "    \n",
    "    #assert BERN.num_labels == len(y_probs)\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "y_pred = np.argmax(y_probs,1)\n",
    "y_pred = aasm_to_wake_sleep(y_pred)\n",
    "\n",
    "# remove y_pred where outside MS definition\n",
    "#_y_pred = remove_invalid_labels(y_pred, 1, fs=200)\n",
    "\n",
    "y_true = BERN.labels\n",
    "\n",
    "# Add predictions using cumulative sleep probabilites \n",
    "yy_probs = np.empty([y_probs.shape[0],2])\n",
    "yy_probs[:,0] = y_probs[:,0]\n",
    "yy_probs[:,1] = 1-y_probs[:,0]#np.sum(y_probs[:,1:],1)\n",
    "yy_pred = np.argmax(yy_probs,1)\n",
    "#_yy_pred = remove_invalid_labels(yy_pred, 1, fs=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19aa07fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highest sleep class vs wake\n",
    "y_bin = np.empty([y_probs.shape[0],2])\n",
    "y_bin[:,0] = y_probs[:,0]\n",
    "high_sleep_idx = np.argmax(y_probs[:,1:5],1) + 1\n",
    "for val in np.unique(high_sleep_idx):\n",
    "    idx = np.where(high_sleep_idx == val)\n",
    "    y_bin[idx,1] = y_probs[idx,val]\n",
    "\n",
    "yb_pred = np.argmax(y_bin,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78a6a94",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(nrows=3, ncols=1, figsize=[9,9], sharex=True)\n",
    "lab_fs = 200\n",
    "pred_fs = 1\n",
    "\n",
    "if lab_fs == 200:\n",
    "    xlab = \"Time [s]\"\n",
    "else:\n",
    "    xlab = \"Sample #\"\n",
    "    \n",
    "# Plot hypnodensity\n",
    "plot_probs(ax1, y_probs, [\"Wake\", \"N1\", \"N2\", \"N3\", \"REM\"], fs = pred_fs)\n",
    "ax1.autoscale(enable=True, axis='both', tight=True)\n",
    "\n",
    "# Plot the predicted hypnogram\n",
    "_,_=plot_label_patches(y_pred, {\"Wake\": 0, \"MS\": 1}, y_min=2/3, ax=ax2, fs= pred_fs)\n",
    "_,_=plot_label_patches(yb_pred, {\"Wake\": 0, \"MS\": 1}, y_min=1/3, y_max=2/3, ax=ax2, fs = pred_fs)\n",
    "hdl,names=plot_label_patches(yy_pred, {\"Wake\": 0, \"MS\": 1}, y_max=1/3, ax=ax2, fs= pred_fs)\n",
    "\n",
    "ax2.legend(hdl, names, loc=1)\n",
    "ax2.autoscale(enable=True, axis='both', tight=True)\n",
    "ax2.plot([*ax2.get_xlim()],[1/3, 1/3],'-k',linewidth=0.5)\n",
    "ax2.plot([*ax2.get_xlim()],[2/3, 2/3],'-k',linewidth=0.5)\n",
    "ax2.set_yticks([0.5/3, 1.5/3, 2.5/3])\n",
    "ax2.set_yticklabels([\"yy_pred\",\"yb_pred\",\"y_pred\"])\n",
    "\n",
    "# Plot the true hypnogram\n",
    "hdl, names = plot_label_patches(y_true, {\"Wake\": 0, \"MS\": 1}, ax = ax3, fs = lab_fs)\n",
    "ax3.autoscale(enable=True, axis='both', tight=True)\n",
    "ax3.legend(hdl, names, loc = 1)\n",
    "ax3.set_xlabel(xlab)\n",
    "clear_output(wait=False)\n",
    "#plt.savefig(fig_path(\"overall\"), format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d19bf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "bp, br, bf, bt = calc_stats(TP,FP,FN)\n",
    "out = {\"Precision\": bp, \"Recall\": br, \"F1-Score\": bf, \"Threshold\": bt}\n",
    "pprint(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5eab92d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "preds = 1*(yy_probs[:,1]>bt)\n",
    "fig, (ax, ax1) = plt.subplots(nrows=2,ncols=1,sharex=True)\n",
    "h,l=plot_label_patches(preds,{\"Wake\":0, \"MS\":1},ax=ax, fs = 200)\n",
    "ax.legend(h,l)\n",
    "h,l=plot_label_patches(y_true, {\"Wake\":0, \"MS\":1},ax=ax1, fs = 200)\n",
    "ax1.legend(h,l,loc=1)\n",
    "ax1.autoscale(enable=True, axis=\"x\", tight=True)\n",
    "[x.set_yticks([]) for x in [ax,ax1]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fad9d6",
   "metadata": {},
   "source": [
    "## U-Sleep with BERN manuscript\n",
    "The BERN scoring criteria is more detailed and conservative than other methods used to identify microsleep episodes (MSE), therfore it deviates from our common definition in numerous ways:\n",
    "\n",
    "__3 Classes of Microsleep__\n",
    "1. _MSE_: Analogous to the common definiton of MSE, but including a requirement of >80% the interval with closed eyes (from videography). This class is additionally scored either unilaterally or bilaterally. This class has a strict and conservative scoring definition.\n",
    "2. _MSEc_: Shares similar features as MSE and has high likelihood of being an MSE, but does not fulfill all criteria of an MSE. Less conservative than MSE.\n",
    "3. _ED_: There is not a clear definition of change in frequency in EEG, quick alterations of regular and irregular activity and morphology of the EEG. When MSE and MSEc criteria are not fulfilled but the episode does not resemble wakefulness. \n",
    "\n",
    "__Duration = 1 to 15+ seconds__\n",
    "+ They define MSE shorter than the 3 second common minimum duration, and reported more than 40% of their scored MSEs shorter than 3 seconds.\n",
    "+ They also define some MSE episodes longer than 15 seconds in rare cases where an AASM sleep is not scored due to the MSE spanning over two epochs.\n",
    "\n",
    "\n",
    "### Mitigation tactics:\n",
    "__Three Classes to MS vs Wake:__\n",
    "* Bern Labels\n",
    "    + We can merge the BERN labels to MS or Wake as follows\n",
    "    | Option  | MS              | Wake             |   |   |\n",
    "    |---------|-----------------|------------------|---|---|\n",
    "    | 1       | {MSE}           | {MSEc, ED, Wake} |   |   |\n",
    "    | 2       | {MSE, MSEc}     | {ED, Wake}       |   |   |\n",
    "    | 3       | {MSE, MSEc, ED} | {ED}             |   |   |\n",
    "    + Options 1. to 3. omit the notion of unilateral events and scores them simply as an event\n",
    "    + In recent automatic classification work by the Bern group, Option 1 showed the highest classification problem and on multiple accounts they state that MSEc is more closely related to MSE and ED with Wake.\n",
    "\n",
    "* U-Sleep output\n",
    "We convert the AASM scoring to MS by\n",
    "    + Choosing the AASM class probability with the highest value, being scored as MS if it is a sleep stage otherwise Wake.\n",
    "    + Thresholding the highest AASM sleep probability {N1, N2, N3 or REM} otherwise as Wake.\n",
    "    + Combining the probabilites of the AASM sleep stages as probabilites of MS and then threholding the cominbed sleep probabilites.\n",
    "\n",
    "__Duration:__\n",
    "* BERN Labels\n",
    "    + Any MS event longer than 15 seconds should be __omitted__ (as it is not a commonly defined MS)\n",
    "    + Combine MS events that are shorter than 3 seconds (MS<3) where the duration from the preceding MS<3 onset is $\\leq$ 3 seconds from the next MS<3.\n",
    "         + _This must be followed with an assertion that the concatenation does not generate MS events > 15 seconds_\n",
    "\n",
    "* U-Sleep output\n",
    "    + Any MS event longer than 15 seconds should be __omitted__ (as it is not a commonly defined MS)\n",
    "    + ??? _Should the same criteria be applied to MS<3_ ???\n",
    "        + _We aim to detect microsleep, so it should be enoguh to know that it occured rather than perfectly overlapping it. So by applying the same criteria as above we are interferring with the predictions and extrapolating?_\n",
    "        \n",
    "### Evaluation:\n",
    "The performance of the predictions will be evaluated analogous to A. Brink-Kjaer et al. (2020)\n",
    "* MS predictions overlapping the true MS events are considered __true positives__\n",
    "* MS predictions not overlapping the true MS events are considered __false positivies__\n",
    "* Wake predictions overlapping true MS eveents are considerded __false negatives__\n",
    "\n",
    "The thresholding will be perfomed using precision recall curves for different thresholds and an F1-score on a training (dev) set and the optimal threshold applied to a validation (test) set. We will use the training split from Malafeev et al. 2020 as dev and validation split as test, to preserve the test for later analysis in the project.\n",
    "\n",
    "The evalaution will be made by concatenating the predictions and true labels of all the recordings\n",
    "\n",
    "Additionally, the overlap metrics can be further quantified as Iou (intersection over union) or average precision.\n",
    "https://medium.com/@timothycarlen/understanding-the-map-evaluation-metric-for-object-detection-a07fe6962cf3\n",
    "\n",
    "A per sample evaluation can also be made, however this would require brutly upsampling the U-sleep predictions to match the labels (200 Hz), and would be a more harsh evaluation metric.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c83b77f",
   "metadata": {},
   "source": [
    "## Manuscript for preliminary U-Sleep evaluation on Bern data\n",
    "The comparison will be made with the results from Skorucack et al., 2020 (RF, SVM, LSTM classifiers).\n",
    "The evaluation used in the reference study:\n",
    "- bMSE vs Wake (also some other classification problems e.g. bMSE vs {Wake, uMSE, uMSEc, uED})\n",
    "- True labels were converted from 200 Hz to 5 Hz (200 ms resolution) using a 9 second median filter with 200 ms step size.\n",
    "- Predictions from the RF and SVM were converted to 5 Hz resolution using same median filter.\n",
    "- MS predictions after resampling which were shorter than 1 second were excluded\n",
    "- Calculate sensitivity, specificity, accuracy, precision, cohen's kappa.\n",
    "    \n",
    "In this analysis, we will use the same dev/test split (53/23) and adapt the U-Sleep output to fit the same evaluation scheme as used in the reference paper.\n",
    "The hyperparameters of the U-Sleep model are:\n",
    "* Data per prediction (prediction rate): 1, 2, 4, 8, 16\n",
    "* Post-procesing of probabilities: y_argmax (not w/ tunable threshold), y_max_sleep, and y_sum_sleep\n",
    "* MS Threshold: 0.025:0.025:1.0\n",
    "\n",
    "Therefore, making 5x3 = 15 models.\n",
    "Since the U-Sleep model is pre-trained, the only \"training\" part is the threshold tuning. The optimal threshold will be determined by the highest f1-score analogous to Brink-KjÃ¦r. The optimal model will be found by using a 5-fold CV validation where a model will be trained (tune threshold) on K-1 folds and validated against the remaining fold. The model with the highest f1-score will be chosen and re-trained on the entire dev set before evaluating it on the test set.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b720a51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_probs(rec):\n",
    "    probs = get_probs(rec)\n",
    "    probs_sum = np.column_stack([probs[:,0], np.sum(probs[:,1:5],axis=1)])\n",
    "    probs_max = np.column_stack([probs[:,0], np.max(probs[:,1:5],axis=1)])\n",
    "    \n",
    "    return probs, probs_sum, probs_max\n",
    "\n",
    "\n",
    "def psuedo_resample(y_org, first_last):\n",
    "    if len(y_org.shape) > 1:\n",
    "        return np.array([np.median(y_org[:,x[0]:x[1]],1) for x in first_last]).T\n",
    "    else:\n",
    "        return np.array([np.median(y_org[x[0]:x[1]]) for x in first_last])\n",
    "    \n",
    "make_first_last = lambda time_pos, hz: np.array([[np.floor(x[0]*hz), np.ceil(x[-1]*hz)+1] for x in time_pos], dtype=int)\n",
    "\n",
    "_map = {\"Wake\": [0,2,3], \"MS\": [1]}\n",
    "_uni = False\n",
    "\n",
    "# thresholds\n",
    "# Initialization\n",
    "tstep = 0.025\n",
    "tstart = 0.025\n",
    "tmax = 1.0\n",
    "tnum = ((tmax - tstart) / tstep) + 1\n",
    "thresholds = np.linspace(tstart,1.0,np.round(tnum).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "7b2a07ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe creation for 8 Hz\n",
      "Dataframe creation for 16 Hz\n",
      "Dataframe creation for 32 Hz\n",
      "Dataframe creation for 64 Hz\n",
      "Dataframe creation for 128 Hz\n"
     ]
    }
   ],
   "source": [
    "## PROCESSING PRELIM ANALYSIS\n",
    "\n",
    "HZ = [8, 16, 32, 64, 128]\n",
    "for hz in HZ:\n",
    "    resampled_labels = dict.fromkeys(all_names)\n",
    "    resampled_first_last = dict.fromkeys(all_names)\n",
    "    entries = []\n",
    "        \n",
    "    print(f\"Dataframe creation for {hz} Hz\")\n",
    "    for edf, lab in zip(all_edf_files, all_labels_files):\n",
    "\n",
    "        _id = edf.replace(\".edf\",\"\")\n",
    "        _type = \"train\" if _id in splits[\"train\"] else \"test\"\n",
    "\n",
    "        _edf = os.path.join(\"edf_data\",edf)\n",
    "        _labels = os.path.join(\"labels\",lab)\n",
    "        _preds = os.path.join(\"predictions\", f\"{hz}_hz\",f\"{_id}.npy\")\n",
    "\n",
    "        _tmp = BernLabels(lab, _map, _uni)\n",
    "        _any_ms = np.sum(_tmp.labels) > 1\n",
    "\n",
    "        _ms_200, _time_pos = _tmp.apply_rolling_func(win=0.2, step=0.2)\n",
    "        _ms_200[_ms_200 == 0.5] == 1\n",
    "        _any_ms_200 = np.sum(_ms_200) > 1\n",
    "\n",
    "        entry = {\"type\": _type, \"id\": _id, \"edf\": _edf, \"labels\": _labels, \"preds\": _preds,\n",
    "                 \"ms\": _any_ms, \"ms_200\": _any_ms_200}\n",
    "        entries.append(entry)\n",
    "\n",
    "        fixed_resampled_labels = _ms_200\n",
    "        fixed_resampled_labels[fixed_resampled_labels==0.5] == 1\n",
    "        \n",
    "        resampled_labels[_id] = np.array(fixed_resampled_labels, dtype=int)\n",
    "        resampled_first_last[_id] = make_first_last(_time_pos, hz)\n",
    "\n",
    "    df = pd.DataFrame.from_records(entries)\n",
    "    df.to_csv(f\"prelim_data/corrected_{hz}_info_df.csv\")\n",
    "\n",
    "#     processed_recs = dict.fromkeys(all_names)\n",
    "#     print(f\"Processing recs for {hz} Hz\")\n",
    "    \n",
    "#     for i, row in df.iterrows():\n",
    "#         print(f\"{i+1}/{df.shape[0]}\")\n",
    "#         p1,p2,p3 = get_all_probs(row.preds)\n",
    "#         fl = resampled_first_last[row.id]\n",
    "\n",
    "#         # Argmax\n",
    "#         preds_argmax = aasm_to_wake_sleep(np.argmax(p1,axis=1))\n",
    "#         resampled_preds_argmax=psuedo_resample(preds_argmax, fl)\n",
    "#         resampled_preds_argmax[resampled_preds_argmax==0.5] = 1\n",
    "        \n",
    "#         # Sum\n",
    "#         preds_sum = np.array([p2[:,1] > t for t in thresholds])*1\n",
    "#         resampled_preds_sum=psuedo_resample(preds_sum, fl)\n",
    "#         resampled_preds_sum[resampled_preds_sum==0.5] = 1\n",
    "        \n",
    "#         # Max\n",
    "#         preds_max = np.array([p3[:,1] > t for t in thresholds])*1\n",
    "#         resampled_preds_max=psuedo_resample(preds_max, fl)\n",
    "#         resampled_preds_max[resampled_preds_max==0.5] = 1\n",
    "            \n",
    "#         # Store\n",
    "#         entry = {\"preds_argmax\": resampled_preds_argmax,\n",
    "#                  \"preds_sum\": resampled_preds_sum,\n",
    "#                  \"preds_max\": resampled_preds_max,\n",
    "#                  \"labels\": resampled_labels[row.id]}\n",
    "#         processed_recs[row.id] = entry\n",
    "#         clear_output(wait=True)\n",
    "\n",
    "\n",
    "#     pickle_file = f'{hz}_processed_recs2.pickle'\n",
    "#     write_to_pickle_file(processed_recs, pickle_file)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "308a185d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collector(collection, ids, key, rm=True):\n",
    "    #print(f\"Removing invalid labels: {rm}\")\n",
    "    i = 0\n",
    "    for k, sub_collection in collection.items():\n",
    "        if k not in ids:\n",
    "            #print(f\"Skipping: {k}\")\n",
    "            continue\n",
    "\n",
    "        v = sub_collection[key]\n",
    "        _y = sub_collection[\"labels\"]\n",
    "        _yy = remove_invalid_labels(_y, min_duration=1, max_duration=np.inf, fs = 5, verbose=False)\n",
    "        \n",
    "        if rm:\n",
    "            v = np.array([remove_invalid_labels(vx,min_duration=1, max_duration=np.inf, fs=5, verbose=False) for vx in v])\n",
    "        if i == 0:\n",
    "            y_hat = v\n",
    "            y = _y\n",
    "        else:\n",
    "            y_hat = np.column_stack([y_hat, v]) if len(y_hat.shape) > 1 else np.hstack([y_hat, v])\n",
    "            y = np.hstack([y, _y])\n",
    "        i += 1\n",
    "    return y_hat, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "bcd8db26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Tracker(object):\n",
    "    \n",
    "    \n",
    "    def __init__(self, k, train_y_true, train_y_hat, val_y_true, val_y_hat):\n",
    "    \n",
    "        self.k = k\n",
    "        \n",
    "        tuning = len(train_y_hat.shape) > 1\n",
    "        \n",
    "        # Calculate training metrics\n",
    "        loop_func = lambda yt, yh, f: [f(yt,y) for y in yh]\n",
    "        static_func = lambda yt, yh, f: f(yt,yh)\n",
    "        func = loop_func if tuning else static_func\n",
    "\n",
    "        self._train_precision = func(train_y_true, train_y_hat, metrics.precision_score)\n",
    "        self._train_recall    = func(train_y_true, train_y_hat, metrics.recall_score)\n",
    "        self._train_f1        = func(train_y_true, train_y_hat, metrics.f1_score)\n",
    "        self._train_kappa     = func(train_y_true, train_y_hat, metrics.cohen_kappa_score)\n",
    "       \n",
    "        if tuning:\n",
    "            opt_idx = np.argmax(self._train_kappa) if len(train_y_hat.shape) > 1 else 0\n",
    "            self.opt_idx = opt_idx\n",
    "            self.opt_threshold = thresholds[opt_idx]\n",
    "\n",
    "            self.train_opt_kappa     = self._train_kappa[opt_idx]\n",
    "            self.train_opt_f1        = self._train_f1[opt_idx]\n",
    "            self.train_opt_recall    = self._train_recall[opt_idx]\n",
    "            self.train_opt_precision = self._train_precision[opt_idx]\n",
    "\n",
    "        else:\n",
    "            self.opt_idx = np.nan\n",
    "            self.opt_threshold = np.nan\n",
    "            \n",
    "            self.train_opt_kappa     = self._train_kappa\n",
    "            self.train_opt_f1        = self._train_f1\n",
    "            self.train_opt_recall    = self._train_recall\n",
    "            self.train_opt_precision = self._train_precision\n",
    "        \n",
    "\n",
    "        # Calculate validation metrics\n",
    "        val_y_opt     = val_y_hat[opt_idx,:] if tuning else val_y_hat\n",
    "        \n",
    "        self.val_precision = metrics.precision_score(val_y_true, val_y_opt)\n",
    "        self.val_recall    = metrics.recall_score(val_y_true, val_y_opt)\n",
    "        self.val_f1        = metrics.f1_score(val_y_true, val_y_opt)\n",
    "        self.val_kappa     = metrics.cohen_kappa_score(val_y_true, val_y_opt)\n",
    "        \n",
    "    def to_dict(self):\n",
    "        d = vars(self)\n",
    "        out = {}\n",
    "        for k,v in d.items():\n",
    "            if not k.startswith(\"_\"):\n",
    "                out[k] = v\n",
    "        return out\n",
    "\n",
    "class EvaluationTracker(object):\n",
    "    \n",
    "    \n",
    "    def __init__(self, y_true, y_hat, opt_idx = None):\n",
    "    \n",
    "        if np.any(opt_idx) and len(y_hat.shape) > 1:\n",
    "            y_hat = y_hat[opt_idx,:]\n",
    "        \n",
    "        try:\n",
    "            tn, fp, fn, tp = metrics.confusion_matrix(y_true, y_hat).ravel()\n",
    "            self.specificity = tn / (tn+fp)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        self.accurcy     = metrics.accuracy_score(y_true, y_hat)\n",
    "        self.precision   = metrics.precision_score(y_true, y_hat)\n",
    "        self.recall      = metrics.recall_score(y_true, y_hat)\n",
    "        self.f1          = metrics.f1_score(y_true, y_hat)\n",
    "        self.kappa       = metrics.cohen_kappa_score(y_true, y_hat)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af1221b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_df = pd.read_csv(\"prelim_data/corrected_8_info_df.csv\")\n",
    "test_df = _df[_df.type==\"test\"].reset_index(drop=True)\n",
    "dev_df = _df[_df.type==\"train\"].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "93ac5b22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K: 4 - Hz: 8 - Method: preds_max\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sindri\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\sindri\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\sindri\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seed = 42\n",
    "k = 5\n",
    "skf = model_selection.StratifiedKFold(k, shuffle=True, random_state=seed)\n",
    "\n",
    "pred_keys = [\"preds_argmax\",\"preds_sum\",\"preds_max\"]\n",
    "HZ = [128, 64, 32, 16, 8]\n",
    "i = 0\n",
    "for hz in HZ:\n",
    "    processed_recs_file = f\"prelim_data/{hz}_processed_recs2.pickle\"\n",
    "    processed_recs = load_pickle_from_file(processed_recs_file)\n",
    "    for pk in pred_keys:\n",
    "        for k, (train_idx, val_idx) in enumerate(skf.split(dev_df.index, dev_df.ms)):   \n",
    "\n",
    "            print(f\"K: {k} - Hz: {hz} - Method: {pk}\")\n",
    "            train_id = dev_df.id[train_idx].values\n",
    "            val_id = dev_df.id[val_idx].values\n",
    "            \n",
    "            train_yhat, train_y = my_collector(collection = processed_recs, ids = train_id, key=pk, rm = True)\n",
    "            val_yhat, val_y     = my_collector(collection = processed_recs, ids = val_id, key=pk, rm = True)\n",
    "\n",
    "            k_tracker = Tracker(k, train_y, train_yhat, val_y, val_yhat)\n",
    "            k_dict = k_tracker.to_dict()\n",
    "            k_dict[\"method\"] = pk\n",
    "            k_dict[\"hz\"] = hz\n",
    "\n",
    "            if i == 0:\n",
    "                k_df = pd.DataFrame(k_dict, index=[i])\n",
    "            else:\n",
    "                k_df = pd.concat([k_df, pd.DataFrame(k_dict, index=[i])])\n",
    "\n",
    "            clear_output(wait=True)\n",
    "            i += 1\n",
    "        k_df.to_csv(\"corrected_prelim2_df.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a158adac",
   "metadata": {},
   "source": [
    "### Diagnostics...\n",
    "Go to post_hoc_analysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b418ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bern_mapping = {\"Wake\": [0,2,3], \"MS\": [1]}\n",
    "# mp = {\"Wake\": 0, \"MS\": 1}\n",
    "# unilateral = False\n",
    "# pat = test_df.iloc[3]\n",
    "# tmp = BernLabels(pat.labels, bern_mapping, include_unilateral=unilateral)\n",
    "# _, time_pos = tmp.apply_rolling_func(win=0.2, step=0.2)\n",
    "# fl = np.array([[np.floor(x[0]*best_hz), np.ceil(x[-1]*best_hz)+1] for x in time_pos], dtype=int)\n",
    "\n",
    "# _, axs = plt.subplots(3,1,sharex=True,figsize=[9,9])\n",
    "\n",
    "# y_probs,y_sum_probs,y_max_probs = get_all_probs(f\"predictions/{best_hz}_hz/{pat.id}.npy\")\n",
    "# plot_probs(ax=axs[0], probs=y_probs, labs=AASM_MAPPING.keys(), fs=best_hz)\n",
    "# axs[0].axhline(1-t.opt_threshold,color=\"k\",linestyle='--')\n",
    "\n",
    "# # show prediction workflow from probabilties, median sampling, short removal\n",
    "# y_pred, _y=my_collector(collection=collection, ids=pat.id, key=best_method)\n",
    "# y_opt = y_pred[t.opt_idx,:]\n",
    "# y_sum_preds = (y_sum_probs[:,1] > t.opt_threshold)*1\n",
    "# y_sum_resampled = np.array([np.median(y_sum_preds[x[0]:x[1]]) for x in fl])\n",
    "# y_sum_resampled[y_sum_resampled == 0.5] = 1\n",
    "# y_sum_resampled_and_removed = remove_invalid_labels(np.copy(y_sum_resampled),  1, min_duration=1, max_duration=np.inf, fs=5)\n",
    "# plot_label_patches(y_sum_preds, mp, ax=axs[1], fs=best_hz, y_min=3/4, y_max=1)\n",
    "# plot_label_patches(y_sum_resampled, mp, ax=axs[1], fs=5, y_min=2/4, y_max=3/4)\n",
    "# plot_label_patches(y_sum_resampled_and_removed, mp, ax=axs[1], fs = 5, y_min=1/4, y_max=2/4)\n",
    "# plot_label_patches(y_opt, mp, ax=axs[1], fs=5, y_min=0, y_max=1/4)\n",
    "# axs[1].set_yticks([0])\n",
    "# axs[1].set_ylabel(f\"Predictions with \\n {t.opt_threshold} threshold\")\n",
    "\n",
    "# tmp.apply_rolling_func(win=0.2, step=0.2,replace=True)\n",
    "# tmp.labels[tmp.labels==0.5] = 1\n",
    "# tmp.apply_time_critera(replace=True, min_duration=1, max_duration=np.inf)\n",
    "# tmp.plot_labels(ax=axs[2], fs=5)\n",
    "\n",
    "\n",
    "# for ax in axs:\n",
    "#     ax.autoscale(tight=True)\n",
    "\n",
    "# #plot_label_patches(y_opt, mp, ax=ax2, fs=5, y_min=i/40, y_max=(i+1)/40)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ce8a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find2(y,target):\n",
    "#     r,c = np.where(y==target)\n",
    "#     idx=np.empty(y.shape)\n",
    "#     idx[r,c] = 1\n",
    "#     islands = np.diff(idx)\n",
    "#     zs = np.zeros(islands.shape[0])\n",
    "#     ffy = np.column_stack([zs, islands, zs])\n",
    "#     diff_idx = np.diff(ffy)\n",
    "\n",
    "#     start_r, start_c = np.where(diff_idx > 0)\n",
    "#     stop_r, stop_c = np.where(diff_idx < 0)\n",
    "\n",
    "#     start = [(start_c[start_r==i]) for i in range(40)]\n",
    "#     stop = [(stop_c[stop_r==i]) for i in range(40)]\n",
    "    \n",
    "#     return start, stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e4ac2722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "624d72e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>55.113333</td>\n",
       "      <td>16881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>157.706667</td>\n",
       "      <td>48214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         duration  count\n",
       "type                    \n",
       "test    55.113333  16881\n",
       "train  157.706667  48214"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from utils import *\n",
    "from scipy.stats import mode\n",
    "\n",
    "df = pd.read_csv(\"prelim_data/corrected_8_info_df.csv\")\n",
    "count = []\n",
    "durs = []\n",
    "tt = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    tmp = BernLabels(row.labels, {\"Wake\": [0,2,3], \"MS\":[1]}, include_unilateral=True)\n",
    "    tmp.apply_rolling_func(win=0.2, step=0.2,replace=True)\n",
    "    tmp.labels[tmp.labels==0.5]=1\n",
    "    start, stop, singles = get_target_label_start_and_stop_indices(tmp.labels, 1)\n",
    "    y=tmp.labels\n",
    "    dur = (stop-start)/tmp.fs\n",
    "    total_dur = np.sum(dur / 60)\n",
    "    count.append(np.sum(y==1))\n",
    "    durs.append(total_dur)\n",
    "    tt.append(row.type)\n",
    "    \n",
    "    \n",
    "#     for j, arr in enumerate([tmp.raw_O1, tmp.raw_O2]):\n",
    "#         start, stop, singles = get_target_label_start_and_stop_indices(arr, 1)\n",
    "#         dur = (stop-start)/tmp.fs\n",
    "#         total_dur = np.sum(dur / 60)\n",
    "#         count.append(len(start))\n",
    "#         durs.append(total_dur)\n",
    "#         tt.append(row.type)\n",
    "#         sides.append(i)\n",
    "    \n",
    "    \n",
    "stats = pd.DataFrame({\"type\": tt, \"duration\": durs, \"count\": count})\n",
    "stats.groupby([\"type\"]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b4634866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 1., 1., 0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.empty([1,10])\n",
    "a[1:5] = 1\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
